how you would go about using the system
16:52
that we built called
16:53
earnest looks something like this from
16:56
the user side we get the job binary that
16:58
they're trying to run and some sort of
17:00
grid of values that we should try for
17:02
building our model itself and then both
17:06
of these are fed into this box and
17:07
inside this box we run a number of
17:10
modules which are going to actually
17:12
perform some of the actions that I
17:13
described earlier in the talk so the
17:15
first action is going to be it's going
17:16
to run experiment design try and figure
17:18
out the optimal set of training data
17:20
points to collect and using those
17:22
training data points it's going to
17:24
launch some training jobs on a cluster
17:25
to run the other key point here is that
17:29
we actually run only a few iterations of
17:31
the job in order to collect some
17:32
training data and using that we can also
17:34
extrapolate how the future iterations
17:36
will look and once we have collected
17:39
this training data we then put it into a
17:40
linear model and the output of this
17:43
linear model gives us this relation with
17:46
how the running time varies as we change
17:49
the number of machines are the input
17:51
size for our job so this gives you an
17:54
end-to-end view of how earnest works
17:56
with that I'm going to go and talk about
17:59
some of the evaluation that we did and
18:01
some of the use case scenarios and also
18:03
how accurate our model is and how long
18:06
it takes to build our model we ran this
18:09
on a number of different workloads
18:10
including SPARC ml lab and some of the
18:13
Keystone pipeline that I showed you
18:14
before and those are the two I'm going
18:16
to have time to talk about in the talk
18:18
today but you can look at the rest of
18:20
them in our paper and in our github page
18:22
that I'll talk about a little bit later
18:25
so let's start with the same like timid
18:28
pipeline that I used as a motivating
18:29
example before and this graph here shows
18:32
the scaling curve for that particular
18:34
workload on our three dot extra-large
18:36
instances when running hundred
18:38
iterations so one of the big advantages
18:41
of having this model is that let's say
18:43
if we had a two hour deadline then we
18:46
could actually pick around 18 machines
18:48
to run this on ec2 instead of picking
18:50
let's say 100 machines or so and thus we
18:53
could get up to like 5x lower cost just
18:55
by knowing that around 18 machines is
18:57
good enough for me to hit my two hour
18:59
deadline for this particular job that we
19:01
are running in terms of accuracy the way
19:06
we mesh in this is that we collect the
19:08
black line here shows what our model
19:10
predicts while the orange dots here show
19:13
the actual running time that we
19:14
collected a number of different
19:16
configurations so we find that our
19:18
prediction error is between like 10 to
19:20
15% for a number of different workloads
19:23
and mostly we think this is good enough
19:25
for such a scale because we are only
19:27
making coarse-grained decisions on how
19:29
many machines to launch or what kind of
19:31
instance to choose and thus predicting
19:33
the exact running time down to a second
19:35
is actually not that important for this
19:37
particular use case we see similar
19:40
patterns on a number of different like
19:42
ml of algorithms as well including
19:44
k-means regression naive bayes and so on
19:47
and these are the actual vs. like
19:49
predicted values where 1.0 meaning that
19:52
we are exactly accurate 0.8 means that
19:54
the prediction was lower by 20% and 1.1
19:58
means it was higher by like 10%
20:00
so in terms of training time out how
20:03
much time did it take to train this
20:05
model this is for the same timid
20:07
pipeline as before we use seven data
20:09
points from experiment designed to Train
20:11
the model and then we ran the whole
20:13
algorithm as well on like hundred
20:15
iterations and what we see here is that
20:17
the overall training time is less than
20:19
like five percent of the overall running
20:21
time and hence it's effective to use
20:23
such an approach to build a model for
20:25
machine learning algorithms a similar
20:28
property can also be seen in like the
20:30
GLM regression algorithm from ml lab
20:33
where we spend less than like 4 percent
20:34
of the time in training a model and we
20:37
get a pretty good error prediction even
20:39
with that much even with just meaning
20:41
that much time the last thing that I'm
20:45
going to present in terms of experiments
20:46
is how useful is this experiment design
20:49
approach that I presented especially
20:50
when it compared to the greedy approach
20:51
which is a lot more simpler and across a
20:55
number of different algorithms what we
20:57
found was that experiment design gave us
20:58
a lot lower prediction error compared to
21:00
the greedy approach and that for some a
21:03
workloads like PCA the greedy the cost
21:05
based approach was like missing the
21:07
entire performance model and we were
21:10
getting really high errors while
21:11
experiment design was able to keep us
21:13
within a small error bracket for a fixed
21:15
budget for when both of them were using
21:17
a fixed budget so there's a lot more
21:21
details that we addressed as a part of
21:23
this research project including trying
21:25
to figure out what can we figure out
21:28
when the model is wrong like let's say
21:29
you we are workload doesn't follow the
21:31
model that I described can we detect
21:33
when the model is wrong automatically
21:35
and also can we extend the model for it
21:37
to cover some other types of workloads
21:38
that might have like some different
21:40
computational communication properties
21:42
and finally we also discuss some like
21:45
strategies in order to mitigate
21:47
stragglers or to deal with like sparse
21:49
data sets and there's a lot of details
21:51
in the paper link that I put about there
21:54
now you might want to try this out as
21:56
well
21:56
so there's a small Python based
21:58
implementation that's open source on the
22:01
github page in ampliar and it just
22:04
provides a Python implementations for
22:06
the experiment design and the linear
22:08
regression based predictor modules and
22:10
there's also an example of like how you
22:12
would use it when you're using let's say
22:14
spark ml ml Lib along with like an RC v1
22:18
data set so there's a small tutorial out
22:20
there as well
22:21
on the github page of Ernest so in
22:25
conclusion there's a looking at the
22:28
vertical of trends today points to the
22:30
fact that a large number of users are
22:32
using what I like to think of as
22:34
advanced analytics in the cloud these
22:36
advanced analytics have like different
22:38
communication computation patterns that
22:40
severely affects scalability and
22:42
especially for more high-level users
22:44
trying to understand what resource
22:46
bottlenecks their workload and how to
22:48
deploy their cluster in a cost-effective
22:50
way is like really challenging to
22:53
address this problem we took this
22:55
approach based on building a performance
22:56
model with low overhead and to do that
23:00
we looked at some of the regular
23:02
structure that's present in these
23:03
workloads and we captured them in this
23:05
linear model and in order to reduce the
23:08
amount of time spent in collecting data
23:09
we use techniques from optimal
23:11
experiment design I'd be more than happy
23:14
to talk to you after the talk if you
23:16
have some workloads or traces that you
23:18
think would fit well with a framework
23:19
like this or if there are some similar
23:22
problems that you're facing in in your
23:24
in your scenario as well with that I'll
23:27
take any questions thank you hi so I saw
23:39
you when you do the optimal experiment
23:42
design you have three points is that
23:43
number of data points that fix all it's
23:48
it's not fixed so so the the output of
23:52
ox out in the optimal experiment design
23:54
let me just go back to that slide
23:55
actually one site so we do this
23:58
relaxation that I didn't quite get a
24:00
chance to talk about so if you notice
24:02
this this lambda variable here which is
24:05
the fraction of times the experiment is
24:07
done it's a value between zero and one
24:09
and what the result of the Occident
24:11
experiment design procedure is is a
24:13
assignment of lambda for each of these
24:15
variables each of these options so there
24:18
is some experiment values that is very
24:20
confident about so let's
24:22
like 1.0 or like 0.9 and some of the
24:24
things are not that useful at all so the
24:26
lambda will be like point zero zero one
24:28
or something like that so we just use a
24:30
threshold so we have a threshold of
24:32
point eight or something like that and
24:33
everything about that we just use that
24:35
as the experiment design training points
24:37
but you can also do this in an online
24:39
fashion whether you sort it by these
24:41
lambda values and you pick the most
24:43
important one first you run it and then
24:45
you look at how how your model is doing
24:47
and then you keep on improving it so so
24:50
that's the approach we use right now but
24:52
we are seeing if there's some ways to
24:53
improve it in the future yeah excellent
25:00
we got another question back here so I'm
25:05
curious would you would you apply this
25:08
if you were like in an elastic
25:09
environments where you can add or remove
25:11
machines that will apply this in like an
25:14
online fashion where you say you're
25:16
you're running a stream processing
25:24
solution for sizing yeah that's a good
25:27
question like but this is a good
25:29
approach to lose sizing so if looking at
25:33
it a slightly higher level so what we're
25:35
doing here is more like a static
25:37
modeling where we're running some
25:39
iterations of the job for training and
25:42
then we're using that to build is like
25:43
model offline and the the elastic sizing
25:48
thing is a more dynamic operation where
25:50
you want let's say like re-evaluate your
25:52
choices every now and then so since we
25:56
were motivated by these machine learning
25:58
jobs we did the static approach because
26:00
there are these long training jobs and
26:01
you can use a few iterations to see how
26:03
well they're doing I think for the
26:05
dynamic ones this this would be a
26:06
reasonable approach if you have
26:07
periodically running it so let's say you
26:09
read on this procedure like every hour
26:11
every two hours or so on but I think
26:13
there are slightly better techniques
26:14
like for example in the inactive
26:18
learning like for example I think in
26:20
statistics it's called like multi-armed
26:22
bandit or something like that where you
26:24
are doing this exploration versus
26:26
exploitation of seeing if I increase
26:28
elasticity how well does it behave and
26:30
then you get back some feedback and
26:31
based on that you adjust it again so I
26:33
think we might be able to do something
26:35
better for
26:35
the online case than just repeatedly
26:37
training like static models but it's
26:40
still an area of research so I don't
26:42
have a very good answer for that that's
26:44
that's great thanks yep come back over
26:48
here hey Dave pellet is from 2 Sigma
26:55
I've noticed that in practice we see
26:58
varying performance across instance
27:00
types based on either time of day or
27:02
even availability zone and so it becomes
27:05
a multi-dimensional problem very quickly
27:07
so have you thought about capturing the
27:09
other dimensions of the problem and then
27:11
the follow-up is how about crowdsourcing
27:13
this data so I could run experiments on
27:15
my cluster and give that data to you to
27:18
inform the model so two parts yeah so
27:20
the second part that would be great
27:22
actually I mean so this we don't have a
27:24
very good way of like I think this I
27:25
mean maybe this is a spark module that
27:28
we can build out which is to collect
27:30
some anonymous data which just gets you
27:32
the resource usage and like the running
27:34
times and so on and just exports that
27:36
like maybe if there's a button that we
27:37
put on the UI that says export logs and
27:39
like ship it you know that would be
27:41
really useful especially for like people
27:43
doing research it's very hard to get
27:44
these realistic traces to answer the
27:47
first part of your question about the
27:49
instance type so that is one of the
27:51
problems in in a sense that so right now
27:53
in this particular paper in this
27:55
particular project the way we handle
27:57
different instance types is we used to
27:59
model every instance type separately so
28:01
in some sense we are building a model
28:03
for this job for this particular
28:05
instance type right and you write that
28:08
if you across instance types we would
28:10
actually build separate models for them
28:11
you know and this quickly becomes a
28:13
little bit of a pain if you have like 50
28:15
different instance types that you're
28:16
like considering so there's some
28:18
follow-up work that we've been working
28:19
on which tries to take a more abstract
28:21
view of the problem which tries to
28:23
handle this curse of like dimensionality
28:25
where you have like five or six
28:26
dimensions to explore the trade of that
28:28
is that the model becomes more complex
28:30
and less interpretable so in in some
28:33
sense here we have a very good
28:34
interpretation of like communication
28:35
computation and so on so we can throw
28:39
more machinery at it so this particular
28:41
approach it's it's it's a paper in NS di
28:43
2017 which is this year it uses Bayesian
28:46
optimization in order to handle these
28:48
like five different dimension
28:49
but so there's some trade-offs between
28:51
the two approach the simple approach
28:53
actually is more understandable and we
28:54
can run it a little bit more efficiently
28:56
but it suffers when you have too many
28:59
dimensions while the more complex ones
29:01
works for better demand more dimensions
29:03
yeah thank you so we are out of time
29:08
here let's start a ten minute break but
29:10
first I just want to thank you very much
29:12
for coming and presenting here giving a
29:15
round of applause here this your problem